{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, fbeta_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, LeakyReLU, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up constants\n",
    "TIMEFRAMES = [7, 14, 30]\n",
    "PREDICTIONS = [1, 5]\n",
    "IMG_TYPES = ['MPA', 'CPA', 'MLC', 'API']\n",
    "\n",
    "# Load labels\n",
    "labels = pd.read_csv('labels/labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop the images using the bounding boxes\n",
    "def crop_image(img_path):\n",
    "    # Load the image in grayscale\n",
    "    img = cv2.imread(img_path, 0)\n",
    "\n",
    "    # Check if the image was loaded correctly\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Image at {img_path} not found. Please check the path.\")\n",
    "\n",
    "    # Use regular expression to match numbers followed by \".png\" at the end of the filename\n",
    "    match = re.search(r'(\\d+)(?=\\.png$)', img_path)\n",
    "    \n",
    "    # Check if we found a match\n",
    "    if match:\n",
    "        # Extract the number from the matched group\n",
    "        number = int(match.group(1))\n",
    "        \n",
    "        # Check if the number is one of the specified values\n",
    "        if number == 7:\n",
    "            # Crop the image using the bounding rectangle\n",
    "            crop = img[105:105+115, 80:80+38]\n",
    "        elif number == 14:\n",
    "            # Crop the image using the bounding rectangle\n",
    "            crop = img[100:100+120, 80:80+85]\n",
    "        elif number == 30:\n",
    "            # Crop the image using the bounding rectangle\n",
    "            crop = img[100:100+120, 80:80+132]\n",
    "        elif number == 90:\n",
    "            # Crop the image using the bounding rectangle\n",
    "            crop = img[100:100+120, 80:80+226]\n",
    "        elif number == 180:\n",
    "            # Crop the image using the bounding rectangle\n",
    "            crop = img[100:100+120, 80:80+414]\n",
    "    return crop\n",
    "\n",
    "# Example usage:\n",
    "filenames = labels['Image'].values.tolist()\n",
    "\n",
    "# Testing the function with the provided list of filenames\n",
    "for name in filenames:\n",
    "    try:\n",
    "        cropped_image = crop_image(name)\n",
    "        # Construct the new path for the cropped image\n",
    "        new_path = name.replace('.png', '_cropped.png')\n",
    "        # Save the cropped image\n",
    "        cv2.imwrite(new_path, cropped_image)\n",
    "    except ValueError as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column called 'Image' that contains the path to the cropped image but only if they \n",
    "labels['Image'] = labels['Image'].str.replace('.png', '_cropped.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and convert an image to grayscale\n",
    "def load_image(image_path):\n",
    "    # Load image in grayscale\n",
    "    image = cv2.imread(str(image_path), cv2.IMREAD_GRAYSCALE)\n",
    "    if image is None:\n",
    "        raise ValueError(f\"Unable to load image at path: {image_path}\")\n",
    "    return image\n",
    "\n",
    "filenames = labels['Image'].values.tolist()\n",
    "\n",
    "images = []\n",
    "\n",
    "for name in filenames:\n",
    "    try:\n",
    "        img = load_image(name)\n",
    "        images.append(img)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "\n",
    "# Add a new column to the labels DataFrame to store the image arrays\n",
    "labels['Image_Array'] = images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by date and train on MSCIWorld only\n",
    "labels = labels[labels['Image'].str.contains('MSCIWorld')]\n",
    "labels['Date'] = labels['Image'].str.extract(r'(\\d{4}-\\d{2}-\\d{2})')\n",
    "labels['Date'] = pd.to_datetime(labels['Date'])\n",
    "labels = labels.sort_values(by='Date') \n",
    "print(labels.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the number of 1s and 0s in the dataset\n",
    "print(labels['Label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn_model(num_blocks, initial_filters=64, input_shape=()):\n",
    "    # Determine the number of blocks based on the input image size\n",
    "    if num_blocks == 2:\n",
    "        dilation_rate = (1, 1)\n",
    "    elif num_blocks == 3:\n",
    "        dilation_rate = (1, 2)\n",
    "    else:\n",
    "        dilation_rate = (1, 3)\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    for i in range(num_blocks):\n",
    "        # Adjust the number of filters\n",
    "        filters = initial_filters * (2 ** i)\n",
    "\n",
    "        # Add Convolutional layer\n",
    "        if i == 0:  # Apply specific strides and dilation rate only for the first layer\n",
    "            model.add(Conv2D(filters, kernel_size=(5, 3), strides=(1, 3), padding='same', \n",
    "                             input_shape=input_shape if i == 0 else None))\n",
    "            model.add(Conv2D(filters, kernel_size=(5, 3), strides=(1, 1), dilation_rate=dilation_rate, padding='same', \n",
    "                             input_shape=input_shape if i == 0 else None))\n",
    "        else:\n",
    "            model.add(Conv2D(filters, kernel_size=(5, 3), padding='same'))\n",
    "\n",
    "        # Add LeakyReLU layer\n",
    "        model.add(LeakyReLU(alpha=0.01))\n",
    "\n",
    "        # Add MaxPooling layer\n",
    "        model.add(MaxPooling2D(pool_size=(2, 1)))\n",
    "\n",
    "    # Add Flatten and Dense layers for final prediction\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    #model.compile(optimizer='adam', loss=focal_loss(gamma=2., alpha=0.25), metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_investment_return(y_pred_binary, lastPrice, futurePrice):\n",
    "    # Calculate Rate of Return RoR for each trade independently\n",
    "    # Long positions\n",
    "    long_ror = []\n",
    "    long_investment = 0\n",
    "    # Short positions\n",
    "    short_ror = []\n",
    "    short_investment = 0\n",
    "\n",
    "    y_pred_binary = y_pred_binary.tolist()\n",
    "    for i in range(len(y_pred_binary)):\n",
    "        # Long position\n",
    "        if y_pred_binary[i][0] == 1:\n",
    "            long_return_i = ((futurePrice[i] - lastPrice[i])/lastPrice[i])\n",
    "            long_ror.append(long_return_i)\n",
    "            long_investment += 100\n",
    "        # Short position\n",
    "        else:\n",
    "            short_return_i = ((lastPrice[i] - futurePrice[i])/lastPrice[i])\n",
    "            short_ror.append(short_return_i)\n",
    "            short_investment += 100\n",
    "    \n",
    "    if long_investment > 0:\n",
    "        # Calculate average long RoR\n",
    "        long_avg_ror = np.mean(long_ror)\n",
    "    else:\n",
    "        long_avg_ror = 0\n",
    "    \n",
    "    if short_investment > 0:\n",
    "        # Calculate average short RoR\n",
    "        short_avg_ror = np.mean(short_ror)\n",
    "    else:\n",
    "        short_avg_ror = 0\n",
    "\n",
    "    return long_avg_ror, long_investment, short_avg_ror, short_investment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate the model for each img_type, timeframe, and prediction\n",
    "\n",
    "evaluation_df = pd.DataFrame(columns=['Image_Type', 'Timeframe', 'Prediction', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Hit_Rate', 'Long_Average_RoR', 'Long_Investment', 'Short_Average_RoR', 'Short_Investment'])\n",
    "\n",
    "for img_type in IMG_TYPES:\n",
    "    for timeframe in TIMEFRAMES:\n",
    "        for prediction in PREDICTIONS:\n",
    "            if prediction <= timeframe:\n",
    "                print(f\"Predicting {prediction} days ahead using {img_type} images with {timeframe} days timeframe.\")\n",
    "                \n",
    "                # Initialize variables for weighted sums and total count\n",
    "                weighted_accuracy_sum = 0\n",
    "                weighted_precision_sum = 0\n",
    "                weighted_recall_sum = 0\n",
    "                weighted_f1_score_sum = 0\n",
    "                weighted_hit_rate_sum = 0\n",
    "                total_predictions = 0\n",
    "                long_avg_ror = []\n",
    "                long_investment = []\n",
    "                short_avg_ror = []\n",
    "                short_investment = []\n",
    "                total_investment = []\n",
    "                total_returns = []\n",
    "                \n",
    "                # Filter your data based on prediction, img_type, and timeframe\n",
    "                data = labels[(labels['TimePrediction'] == prediction) &\n",
    "                              (labels['Image'].str.contains(f'/{img_type}/')) &\n",
    "                              (labels['Image'].str.contains(f'_{timeframe}_'))]\n",
    "                data = data.reset_index(drop=True)  # Reset the index to maintain temporal order\n",
    "                \n",
    "                # Create and compile your CNN model based on timeframe\n",
    "                if timeframe == 7:\n",
    "                    model = create_cnn_model(2, input_shape=(115, 38, 1))\n",
    "                elif timeframe == 14:\n",
    "                    model = create_cnn_model(2, input_shape=(120, 85, 1))\n",
    "                elif timeframe == 30:\n",
    "                    model = create_cnn_model(2, input_shape=(120, 132, 1))\n",
    "                \n",
    "                X = np.array(data['Image_Array'].tolist()) / 255.0\n",
    "                lastPrice = data['LastPrice'].tolist()\n",
    "                futurePrice = data['FuturePrice'].tolist()\n",
    "                y = data['Label'].values\n",
    "\n",
    "                # Define the rolling window size for training within the initial training set\n",
    "                window_size = len(X) / 10\n",
    "\n",
    "                # Early stopping callback\n",
    "                early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "                for window in range(5):\n",
    "                    start = 0\n",
    "                    if window == 0:\n",
    "                        end = int(len(X) * 0.4)\n",
    "                    else:\n",
    "                        end = end + int(window_size)\n",
    "\n",
    "                    # Buffer = window size\n",
    "                    # Define end of buffer\n",
    "                    buffer_end = end + int(window_size)\n",
    "\n",
    "                    # Ensure buffer_end does not exceed the length of the initial training set\n",
    "                    buffer_end = min(buffer_end, len(X))\n",
    "\n",
    "                    # Split the data into training and test sets in temporal order\n",
    "                    X_train_temp = X[start:end]\n",
    "                    y_train_temp = y[start:end]\n",
    "                    lastPrice_train_temp = lastPrice[start:end]\n",
    "                    futurePrice_train_temp = futurePrice[start:end]\n",
    "\n",
    "                    X_test_temp = X[buffer_end:] if window == 4 else X[buffer_end:buffer_end+int(window_size*2)]\n",
    "                    y_test_temp = y[buffer_end:] if window == 4 else y[buffer_end:buffer_end+int(window_size*2)]\n",
    "                    lastPrice_test_temp = lastPrice[buffer_end:] if window == 4 else lastPrice[buffer_end:buffer_end+int(window_size*2)]\n",
    "                    futurePrice_test_temp = futurePrice[buffer_end:] if window == 4 else futurePrice[buffer_end:buffer_end+int(window_size*2)]\n",
    "\n",
    "                    model.fit(X_train_temp, y_train_temp, batch_size=32, epochs=10, validation_split=0.2)  \n",
    "                \n",
    "                    # Evaluate the model\n",
    "                    y_pred = model.predict(X_test_temp)\n",
    "                    # Convert predictions to binary: if > 0.5 then 1 else 0\n",
    "                    y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "                    num_predictions = len(y_test_temp)\n",
    "                    \n",
    "                    # Calculate the performance metrics for the current epoch\n",
    "                    current_accuracy = accuracy_score(y_test_temp, y_pred_binary)\n",
    "                    current_precision = precision_score(y_test_temp, y_pred_binary)\n",
    "                    current_recall = recall_score(y_test_temp, y_pred_binary)\n",
    "                    current_f1_score = fbeta_score(y_test_temp, y_pred_binary, beta=1)\n",
    "\n",
    "                    y_test_array = y_test_temp.ravel()  # Convert y_test to a 1D NumPy array if it's a pandas Series\n",
    "                    correct_predictions = np.sum(y_pred_binary.ravel() == y_test_array)\n",
    "                    current_hit_rate = (correct_predictions / num_predictions)\n",
    "\n",
    "                    # Update the weighted sums and total count\n",
    "                    weighted_accuracy_sum += current_accuracy * num_predictions\n",
    "                    weighted_precision_sum += current_precision * num_predictions\n",
    "                    weighted_recall_sum += current_recall * num_predictions\n",
    "                    weighted_f1_score_sum += current_f1_score * num_predictions\n",
    "                    weighted_hit_rate_sum += current_hit_rate * num_predictions\n",
    "                    total_predictions += num_predictions\n",
    "\n",
    "                    long_avg_ror_temp, long_investment_temp, short_avg_ror_temp, short_investment_temp = get_investment_return(y_pred_binary, lastPrice_test_temp, futurePrice_test_temp)\n",
    "                    # Total investment\n",
    "                    long_avg_ror.append(long_avg_ror_temp)\n",
    "                    long_investment.append(long_investment_temp)\n",
    "                    short_avg_ror.append(short_avg_ror_temp)\n",
    "                    short_investment.append(short_investment_temp)\n",
    "                \n",
    "                # replace nan values with 0\n",
    "                long_avg_ror = [0 if np.isnan(x) else x for x in long_avg_ror]\n",
    "                long_investment = [0 if np.isnan(x) else x for x in long_investment]\n",
    "                short_avg_ror = [0 if np.isnan(x) else x for x in short_avg_ror]\n",
    "                short_investment = [0 if np.isnan(x) else x for x in short_investment]\n",
    "\n",
    "                # Calculate the weighted averages after the epoch loop\n",
    "                weighted_average_accuracy = weighted_accuracy_sum / total_predictions\n",
    "                weighted_average_precision = weighted_precision_sum / total_predictions\n",
    "                weighted_average_recall = weighted_recall_sum / total_predictions\n",
    "                weighted_average_f1_score = weighted_f1_score_sum / total_predictions\n",
    "                weighted_average_hit_rate = weighted_hit_rate_sum / total_predictions\n",
    "                            \n",
    "                print(\"Evaluation Metrics:\")\n",
    "                print(f\"Accuracy: {weighted_average_accuracy}\")\n",
    "                print(f\"Precision: {weighted_average_precision}\")\n",
    "                print(f\"Recall: {weighted_average_recall}\")\n",
    "                print(f\"F1 Score: {weighted_average_f1_score}\")\n",
    "                print(f\"Hit Rate: {weighted_average_hit_rate}\")\n",
    "                print(f\"Average Long RoR: {np.mean(long_avg_ror)}\")\n",
    "                print(f\"Long Investment: {np.mean(long_investment)}\")\n",
    "                print(f\"Average Short RoR: {np.mean(short_avg_ror)}\")\n",
    "                print(f\"Short Investment: {np.mean(short_investment)}\")\n",
    "\n",
    "                \n",
    "                # Add the evaluation metrics to the DataFrame\n",
    "                evaluation_df = evaluation_df.append({\n",
    "                    'Image_Type': img_type,\n",
    "                    'Timeframe': timeframe,\n",
    "                    'Prediction': prediction,\n",
    "                    'Accuracy': weighted_average_accuracy,\n",
    "                    'Precision': weighted_average_precision,\n",
    "                    'Recall': weighted_average_recall,\n",
    "                    'F1_Score': weighted_average_f1_score,\n",
    "                    'Hit_Rate': weighted_average_hit_rate,\n",
    "                    'Long_Average_RoR': np.mean(long_avg_ror),\n",
    "                    'Long_Investment': np.mean(long_investment),\n",
    "                    'Short_Average_RoR': np.mean(short_avg_ror),\n",
    "                    'Short_Investment': np.mean(short_investment)\n",
    "                }, ignore_index=True)\n",
    "                \n",
    "                # Save the model\n",
    "                model.save(f\"models/MSCIWorld_{img_type}_{timeframe}_{prediction}.h5\")\n",
    "                print(f\"Model saved as MSCIWorld_{img_type}_{timeframe}_{prediction}.h5\")\n",
    "                print(\"--------------------------------------------------\")\n",
    "\n",
    "# Save the evaluation DataFrame to a CSV file\n",
    "evaluation_df.to_csv('evaluation/separate/MSCIWorld_evaluation_scores.csv', index=False)\n",
    "print(\"Evaluation scores saved to 'evaluation/MSCIWorld_separate/evaluation_scores.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on all images types and evaluate them based on timeframe and prediction\n",
    "\n",
    "evaluation_df = pd.DataFrame(columns=['Timeframe', 'Prediction', 'Accuracy', 'Precision', 'Recall', 'F1_Score', 'Hit_Rate', 'Long_Average_RoR', 'Long_Investment', 'Short_Average_RoR', 'Short_Investment'])\n",
    "\n",
    "for timeframe in TIMEFRAMES:\n",
    "    for prediction in PREDICTIONS:\n",
    "        if prediction <= timeframe:\n",
    "            print(f\"Predicting {prediction} days ahead with {timeframe} days timeframe.\")\n",
    "\n",
    "            # Initialize variables for weighted sums and total count\n",
    "            weighted_accuracy_sum = 0\n",
    "            weighted_precision_sum = 0\n",
    "            weighted_recall_sum = 0\n",
    "            weighted_f1_score_sum = 0\n",
    "            weighted_hit_rate_sum = 0\n",
    "            total_predictions = 0\n",
    "            long_avg_ror = []\n",
    "            long_investment = []\n",
    "            short_avg_ror = []\n",
    "            short_investment = []\n",
    "            total_investment = []\n",
    "            total_returns = []\n",
    "            \n",
    "            # Filter your data based on prediction, img_type, and timeframe\n",
    "            data = labels[(labels['TimePrediction'] == prediction) & (labels['Image'].str.contains(f'_{timeframe}_'))]\n",
    "            data = data.reset_index(drop=True)  # Reset the index to maintain temporal order\n",
    "            \n",
    "            # Create and compile your CNN model based on timeframe\n",
    "            if timeframe == 7:\n",
    "                model = create_cnn_model(2, input_shape=(115, 38, 1))\n",
    "            elif timeframe == 14:\n",
    "                model = create_cnn_model(2, input_shape=(120, 85, 1))\n",
    "            elif timeframe == 30:\n",
    "                model = create_cnn_model(2, input_shape=(120, 132, 1))\n",
    "            #elif timeframe == 90:\n",
    "                #model = create_cnn_model(3, input_shape=(120, 226, 1))\n",
    "            \n",
    "            X = np.array(data['Image_Array'].tolist()) / 255.0\n",
    "            lastPrice = data['LastPrice'].tolist()\n",
    "            futurePrice = data['FuturePrice'].tolist()\n",
    "            y = data['Label'].values\n",
    "\n",
    "            # Define the rolling window size for training within the initial training set\n",
    "            window_size = len(X) / 20\n",
    "\n",
    "            # Early stopping callback\n",
    "            early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "            for window in range(10):\n",
    "                start = 0\n",
    "                if window == 0:\n",
    "                    end = int(len(X) * 0.4)\n",
    "                else:\n",
    "                    end = end + int(window_size)\n",
    "\n",
    "                # Buffer = window size\n",
    "                # Define end of buffer\n",
    "                buffer_end = end + int(window_size)\n",
    "\n",
    "                # Ensure buffer_end does not exceed the length of the initial training set\n",
    "                buffer_end = min(buffer_end, len(X))\n",
    "\n",
    "                # Split the data into training and test sets in temporal order\n",
    "                X_train_temp = X[start:end]\n",
    "                y_train_temp = y[start:end]\n",
    "                lastPrice_train_temp = lastPrice[start:end]\n",
    "                futurePrice_train_temp = futurePrice[start:end]\n",
    "\n",
    "                X_test_temp = X[buffer_end:] if window == 4 else X[buffer_end:buffer_end+int(window_size*2)]\n",
    "                y_test_temp = y[buffer_end:] if window == 4 else y[buffer_end:buffer_end+int(window_size*2)]\n",
    "                lastPrice_test_temp = lastPrice[buffer_end:] if window == 4 else lastPrice[buffer_end:buffer_end+int(window_size*2)]\n",
    "                futurePrice_test_temp = futurePrice[buffer_end:] if window == 4 else futurePrice[buffer_end:buffer_end+int(window_size*2)]\n",
    "\n",
    "                #model.fit(X_train_temp, y_train_temp, batch_size=32, epochs=10, validation_split=0.2, class_weight=class_weight_dict) \n",
    "                model.fit(X_train_temp, y_train_temp, batch_size=32, epochs=10, validation_split=0.2)  \n",
    "            \n",
    "                # Evaluate the model\n",
    "                y_pred = model.predict(X_test_temp)\n",
    "                # Convert predictions to binary: if > 0.5 then 1 else 0\n",
    "                y_pred_binary = np.where(y_pred > 0.5, 1, 0)\n",
    "\n",
    "                num_predictions = len(y_test_temp)\n",
    "                \n",
    "                # Calculate the performance metrics for the current epoch\n",
    "                current_accuracy = accuracy_score(y_test_temp, y_pred_binary)\n",
    "                current_precision = precision_score(y_test_temp, y_pred_binary)\n",
    "                current_recall = recall_score(y_test_temp, y_pred_binary)\n",
    "                current_f1_score = fbeta_score(y_test_temp, y_pred_binary, beta=1)\n",
    "\n",
    "                y_test_array = y_test_temp.ravel()  # Convert y_test to a 1D NumPy array if it's a pandas Series\n",
    "                correct_predictions = np.sum(y_pred_binary.ravel() == y_test_array)\n",
    "                current_hit_rate = (correct_predictions / num_predictions)\n",
    "\n",
    "                # Update the weighted sums and total count\n",
    "                weighted_accuracy_sum += current_accuracy * num_predictions\n",
    "                weighted_precision_sum += current_precision * num_predictions\n",
    "                weighted_recall_sum += current_recall * num_predictions\n",
    "                weighted_f1_score_sum += current_f1_score * num_predictions\n",
    "                weighted_hit_rate_sum += current_hit_rate * num_predictions\n",
    "                total_predictions += num_predictions\n",
    "\n",
    "                long_avg_ror_temp, long_investment_temp, short_avg_ror_temp, short_investment_temp = get_investment_return(y_pred_binary, lastPrice_test_temp, futurePrice_test_temp)\n",
    "                # Total investment\n",
    "                long_avg_ror.append(long_avg_ror_temp)\n",
    "                long_investment.append(long_investment_temp)\n",
    "                short_avg_ror.append(short_avg_ror_temp)\n",
    "                short_investment.append(short_investment_temp)\n",
    "            \n",
    "            # replace nan values with 0\n",
    "            long_avg_ror = [0 if np.isnan(x) else x for x in long_avg_ror]\n",
    "            long_investment = [0 if np.isnan(x) else x for x in long_investment]\n",
    "            short_avg_ror = [0 if np.isnan(x) else x for x in short_avg_ror]\n",
    "            short_investment = [0 if np.isnan(x) else x for x in short_investment]\n",
    "\n",
    "            # Calculate the weighted averages after the epoch loop\n",
    "            weighted_average_accuracy = weighted_accuracy_sum / total_predictions\n",
    "            weighted_average_precision = weighted_precision_sum / total_predictions\n",
    "            weighted_average_recall = weighted_recall_sum / total_predictions\n",
    "            weighted_average_f1_score = weighted_f1_score_sum / total_predictions\n",
    "            weighted_average_hit_rate = weighted_hit_rate_sum / total_predictions\n",
    "                        \n",
    "            print(\"Evaluation Metrics:\")\n",
    "            print(f\"Accuracy: {weighted_average_accuracy}\")\n",
    "            print(f\"Precision: {weighted_average_precision}\")\n",
    "            print(f\"Recall: {weighted_average_recall}\")\n",
    "            print(f\"F1 Score: {weighted_average_f1_score}\")\n",
    "            print(f\"Hit Rate: {weighted_average_hit_rate}\")\n",
    "            print(f\"Average Long RoR: {np.mean(long_avg_ror)}\")\n",
    "            print(f\"Long Investment: {np.mean(long_investment)}\")\n",
    "            print(f\"Average Short RoR: {np.mean(short_avg_ror)}\")\n",
    "            print(f\"Short Investment: {np.mean(short_investment)}\")\n",
    "\n",
    "            \n",
    "            # Add the evaluation metrics to the DataFrame\n",
    "            evaluation_df = evaluation_df.append({\n",
    "                'Timeframe': timeframe,\n",
    "                'Prediction': prediction,\n",
    "                'Accuracy': weighted_average_accuracy,\n",
    "                'Precision': weighted_average_precision,\n",
    "                'Recall': weighted_average_recall,\n",
    "                'F1_Score': weighted_average_f1_score,\n",
    "                'Hit_Rate': weighted_average_hit_rate,\n",
    "                'Long_Average_RoR': np.mean(long_avg_ror),\n",
    "                'Long_Investment': np.mean(long_investment),\n",
    "                'Short_Average_RoR': np.mean(short_avg_ror),\n",
    "                'Short_Investment': np.mean(short_investment),\n",
    "            }, ignore_index=True)\n",
    "\n",
    "            # Save the model\n",
    "            model.save(f\"models/MSCIWorld_combined_{timeframe}_{prediction}.h5\")\n",
    "            print(f\"Model saved as MSCIWorld_combined_{timeframe}_{prediction}.h5\")\n",
    "            print(\"--------------------------------------------------\")\n",
    "\n",
    "# Save the evaluation DataFrame to a CSV file\n",
    "evaluation_df.to_csv('evaluation/combined/MSCIWorld_rolling_evaluation_scores_combined.csv', index=False)\n",
    "print(\"Evaluation scores saved to 'evaluation/combined/MSCIWorld_rolling_model_evaluation_scores.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiss",
   "language": "python",
   "name": "aiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
