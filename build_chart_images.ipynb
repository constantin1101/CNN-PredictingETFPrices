{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRANSFER = True\n",
    "\n",
    "if TRANSFER:\n",
    "    # Tranfer learning\n",
    "    ijr = pd.read_csv(\"data/transfer/IJR.csv\")\n",
    "    iwm = pd.read_csv(\"data/transfer/IWM.csv\")\n",
    "    tlt = pd.read_csv(\"data/transfer/TLT.csv\")\n",
    "\n",
    "else:\n",
    "    botz = pd.read_csv(\"data/BOTZ.csv\")\n",
    "    spy = pd.read_csv(\"data/SPY.csv\")\n",
    "    iwda = pd.read_csv(\"data/IWDA.AS.csv\")\n",
    "    dax = pd.read_csv(\"data/DAX.csv\")\n",
    "    eem = pd.read_csv(\"data/EEM.csv\")\n",
    "    ixic = pd.read_csv(\"data/IXIC.csv\")\n",
    "    icln = pd.read_csv(\"data/ICLN.csv\")\n",
    "    dia = pd.read_csv(\"data/DIA.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the RSI\n",
    "def calculate_rsi(data, window=14):\n",
    "    delta = data['Close'].diff()\n",
    "    up = delta.clip(lower=0)\n",
    "    down = -1 * delta.clip(upper=0)\n",
    "\n",
    "    ma_up = up.rolling(window=window).mean()\n",
    "    ma_down = down.rolling(window=window).mean()\n",
    "\n",
    "    rsi = 100 - (100 / (1 + ma_up / ma_down))\n",
    "    return rsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_financial_data(data):\n",
    "    # Convert the 'Date' column in the DataFrame to Timestamp objects\n",
    "    data['Date'] = pd.to_datetime(data['Date'])\n",
    "\n",
    "    # Calculating 5 and 30-day SMA\n",
    "    data['SMA_5'] = data['Close'].rolling(window=5).mean()\n",
    "    data['SMA_30'] = data['Close'].rolling(window=30).mean()\n",
    "\n",
    "    # Calculating 20-day SMA for Bollinger Bands\n",
    "    data['SMA_20'] = data['Close'].rolling(window=20).mean()\n",
    "    # Calculating the standard deviation\n",
    "    data['STD_20'] = data['Close'].rolling(window=20).std()\n",
    "    # Calculating Upper and Lower Bollinger Bands\n",
    "    data['Upper_Band'] = data['SMA_20'] + (data['STD_20'] * 2)\n",
    "    data['Lower_Band'] = data['SMA_20'] - (data['STD_20'] * 2)\n",
    "\n",
    "    # Normalize volume bars to the range [0, 1] for better visual consistency\n",
    "    data['normalized_volume'] = (data['Volume'] - data['Volume'].min()) / (data['Volume'].max() - data['Volume'].min())\n",
    "\n",
    "    # Calculate daily returns for Volatility\n",
    "    data['Daily_Return'] = data['Close'].pct_change()\n",
    "\n",
    "    # Calculate rolling 30-day Volatility\n",
    "    data['Volatility_30'] = data['Daily_Return'].rolling(window=30).std()\n",
    "\n",
    "    # Calculate RSI\n",
    "    data['RSI'] = calculate_rsi(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "etfs = {}\n",
    "\n",
    "if TRANSFER:\n",
    "    ijr = prepare_financial_data(ijr)\n",
    "    etfs[\"SmallCap\"] = ijr\n",
    "    iwm = prepare_financial_data(iwm)\n",
    "    etfs[\"Russell2000\"] = iwm\n",
    "    tlt = prepare_financial_data(tlt)\n",
    "    etfs[\"Treasury\"] = tlt\n",
    "\n",
    "else:\n",
    "    spy = prepare_financial_data(spy)\n",
    "    etfs[\"S&P500\"] = spy\n",
    "    botz = prepare_financial_data(botz)\n",
    "    etfs[\"AI&Robotics\"] = botz\n",
    "    icln = prepare_financial_data(iwda)\n",
    "    etfs[\"MSCIWorld\"] = iwda\n",
    "    dax = prepare_financial_data(dax)\n",
    "    etfs[\"DAX\"] = dax\n",
    "    eem = prepare_financial_data(eem)\n",
    "    etfs[\"EmergingMarkets\"] = eem\n",
    "    ixic = prepare_financial_data(ixic)\n",
    "    etfs[\"NASDAQ\"] = ixic\n",
    "    dia = prepare_financial_data(dia)\n",
    "    etfs[\"DowJones\"] = dia\n",
    "    icln = prepare_financial_data(icln)\n",
    "    etfs[\"CleanEnergy\"] = icln"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_chart(fig, etf, start_date, timeframe, chart_type):\n",
    "    # Specify the file path where you want to save the PNG image\n",
    "    file_path = \"images/\" + chart_type + \"/\" + etf + \"_\" + str(start_date) + \"_\" + str(timeframe) + \".png\"\n",
    "\n",
    "    img = fig.to_image(\"png\")\n",
    "\n",
    "    # Save the PNG image to the specified file\n",
    "    with open(file_path, \"wb\") as f:\n",
    "        f.write(img)\n",
    "\n",
    "    return file_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_label(all_data, image_name, end_date, time_of_prediction):\n",
    "    # Calculate the target date by adding the prediction offset to end_date\n",
    "    target_date = pd.to_datetime(end_date) + pd.DateOffset(days=time_of_prediction)\n",
    "\n",
    "    # Filter 'Close' values where 'Date' equals the target_date\n",
    "    price_to_predict = all_data[all_data['Date'] == target_date]['Close']\n",
    "\n",
    "    target_date = target_date + pd.DateOffset(days=1)\n",
    "\n",
    "    # Loop until you find a non-empty price_to_predict or until target_date exceeds the maximum date\n",
    "    while target_date <= all_data['Date'].max():\n",
    "        price_to_predict = all_data[all_data['Date'] == target_date]['Close']\n",
    "        \n",
    "        if not price_to_predict.empty:\n",
    "            break\n",
    "        \n",
    "        target_date = target_date + pd.DateOffset(days=1)\n",
    "\n",
    "    # If target_date exceeds the maximum date, handle the situation (e.g., raise an exception)\n",
    "    if target_date > all_data['Date'].max():\n",
    "        raise ValueError(\"No data available for prediction within the specified timeframe\")\n",
    "\n",
    "    # Calculate if the price goes up (1) or down (0) after prediction_days\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    # Loop until you find a non-empty price_at_end or until target_date exceeds the maximum date\n",
    "    while end_date <= all_data['Date'].max():\n",
    "        price_at_end = all_data[all_data['Date'] == pd.to_datetime(end_date)]['Close']\n",
    "        \n",
    "        if not price_at_end.empty:\n",
    "            break\n",
    "        \n",
    "        end_date = end_date + pd.DateOffset(days=1)\n",
    "\n",
    "    # If target_date exceeds the maximum date, handle the situation (e.g., raise an exception)\n",
    "    if end_date > all_data['Date'].max():\n",
    "        raise ValueError(\"No data available for prediction within the specified timeframe\")\n",
    "    \n",
    "    # Calculate the label\n",
    "    label = 1 if price_at_end.iloc[0] < price_to_predict.iloc[0] else 0\n",
    "\n",
    "    # Append the image name and label to the CSV file\n",
    "    if TRANSFER:\n",
    "        csv_file = 'transfer_labels.csv'    \n",
    "    else:\n",
    "        csv_file = 'labels.csv'\n",
    "    data_to_append = pd.DataFrame({'Image': [image_name], \n",
    "                                   'TimePrediction': [time_of_prediction], \n",
    "                                    'LastPrice': [price_at_end.iloc[0]], \n",
    "                                   'FuturePrice': [price_to_predict.iloc[0]],\n",
    "                                   'Label': [label]})\n",
    "    if os.path.exists(csv_file):\n",
    "        data_to_append.to_csv(csv_file, mode='a', header=False, index=False)\n",
    "    else:\n",
    "        data_to_append.to_csv(csv_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_black_white_ohlc(etf, data, start_date, timeframe):\n",
    "    # Filter the data for the specified date range\n",
    "    data = data[(data['Date'] >= pd.to_datetime(start_date)) \n",
    "                & (data['Date'] <= pd.to_datetime(start_date) + pd.DateOffset(days=timeframe))]\n",
    "\n",
    "    # Create figure with secondary y-axis for volume\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add 30-day Moving Average\n",
    "    fig.add_trace(go.Scatter(x=data['Date'], y=data['SMA_30'], mode='lines', name='30-day SMA', line=dict(color='white'), showlegend=False))\n",
    "\n",
    "    # Normalize volume bars to the range [0, 1] for better visual consistency\n",
    "    normalized_volume = (data['Volume'] - data['Volume'].min()) / (data['Volume'].max() - data['Volume'].min())\n",
    "    # Add volume trace as bars\n",
    "    fig.add_trace(go.Bar(x=data['Date'], y=normalized_volume, name='Volume', marker_color='white', yaxis='y2', showlegend=False))\n",
    "\n",
    "    # Add traces for Candlestick\n",
    "    fig.add_trace(go.Candlestick(x=data['Date'], open=data['Open'], high=data['High'], low=data['Low'], close=data['Close'],\n",
    "                                increasing_line_color='white', decreasing_line_color='white', name=\"Candlestick\", showlegend=False))\n",
    "\n",
    "   # Update layout to remove legends, axis descriptions, tickmarks, numbers, and dates\n",
    "    fig.update_layout(\n",
    "        showlegend=False,  # Remove the legend\n",
    "        xaxis=dict(\n",
    "            title=\"\",  # Remove x-axis description\n",
    "            tickvals=[],  # Remove x-axis tick values\n",
    "            showticklabels=False  # Remove x-axis tick labels\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"\",  # Remove y-axis description\n",
    "            tickvals=[],  # Remove y-axis tick values\n",
    "            showticklabels=False  # Remove y-axis tick labels\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            range=[0, 4],  # Adjusted y-axis range for volume\n",
    "            overlaying='y',\n",
    "            title=\"\",  # Remove y-axis2 description\n",
    "            tickvals=[],  # Remove y-axis2 tick values\n",
    "            showticklabels=False  # Remove y-axis2 tick labels\n",
    "        ),\n",
    "        xaxis_rangeslider_visible=False,  # Remove x-axis rangeslider\n",
    "        paper_bgcolor='black',\n",
    "        plot_bgcolor='black'\n",
    "    )\n",
    "\n",
    "    # Scale image based on timeframe\n",
    "    if timeframe == 14:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=250\n",
    "        )\n",
    "    elif timeframe == 30:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=300\n",
    "        )\n",
    "    elif timeframe == 90:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=400\n",
    "        )\n",
    "    elif timeframe == 180:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=600\n",
    "        )\n",
    "    else:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=800\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Show the figure\n",
    "    image_name = save_chart(fig, etf, start_date, timeframe, \"OHLC\")\n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_colored_ohlc(etf, data, start_date, timeframe):\n",
    "    # Filter the data for the specified date range\n",
    "    data = data[(data['Date'] >= pd.to_datetime(start_date)) \n",
    "                & (data['Date'] <= pd.to_datetime(start_date) + pd.DateOffset(days=timeframe))]\n",
    "\n",
    "    # Create figure with secondary y-axis for volume\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add volume trace as bars\n",
    "    fig.add_trace(go.Bar(x=data['Date'], y=data['normalized_volume'], name='Volume', marker_color='white', yaxis='y2'))\n",
    "\n",
    "    # Add traces for Bollinger Bands, SMA, and Candlestick\n",
    "    fig.add_trace(go.Scatter(x=data['Date'], y=data['Upper_Band'], mode='lines', name='Upper Bollinger Band',\n",
    "                            line=dict(width=1, color=\"blue\")))\n",
    "    fig.add_trace(go.Scatter(x=data['Date'], y=data['Lower_Band'], mode='lines', name='Lower Bollinger Band',\n",
    "                            fill='tonexty', fillcolor='rgba(173,216,230, 0.2)', line=dict(width=1, color=\"blue\")))\n",
    "    fig.add_trace(go.Scatter(x=data['Date'], y=data['SMA_20'], mode='lines', name='20-day SMA', line=dict(width=1, color=\"white\")))\n",
    "    fig.add_trace(go.Candlestick(x=data['Date'], open=data['Open'], high=data['High'], low=data['Low'], close=data['Close'],\n",
    "                                name=\"Candlestick\"))\n",
    "\n",
    "    # Update layout to remove legends, axis descriptions, tickmarks, numbers, and dates\n",
    "    fig.update_layout(\n",
    "        showlegend=False,  # Remove the legend\n",
    "        xaxis=dict(\n",
    "            title=\"\",  # Remove x-axis description\n",
    "            tickvals=[],  # Remove x-axis tick values\n",
    "            showticklabels=False  # Remove x-axis tick labels\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "            title=\"\",  # Remove y-axis description\n",
    "            tickvals=[],  # Remove y-axis tick values\n",
    "            showticklabels=False  # Remove y-axis tick labels\n",
    "        ),\n",
    "        yaxis2=dict(\n",
    "            range=[0, 4],  # Adjusted y-axis range for volume\n",
    "            overlaying='y',\n",
    "            title=\"\",  # Remove y-axis2 description\n",
    "            tickvals=[],  # Remove y-axis2 tick values\n",
    "            showticklabels=False  # Remove y-axis2 tick labels\n",
    "        ),\n",
    "        xaxis_rangeslider_visible=False,  # Remove x-axis rangeslider\n",
    "        paper_bgcolor='black',\n",
    "        plot_bgcolor='black'\n",
    "    )\n",
    "\n",
    "    # Scale image based on timeframe\n",
    "    if timeframe == 14:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=250\n",
    "        )\n",
    "    elif timeframe == 30:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=300\n",
    "        )\n",
    "    elif timeframe == 90:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=400\n",
    "        )\n",
    "    elif timeframe == 180:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=600\n",
    "        )\n",
    "    else:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=800\n",
    "        )\n",
    "\n",
    "    # Save image and label\n",
    "    image_name = save_chart(fig, etf, start_date, timeframe, \"ColoredOHLC\")\n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_ma_vol_rsi(etf, data, start_date, timeframe):\n",
    "    # Filter the data for the specified date range\n",
    "    data = data[(data['Date'] >= pd.to_datetime(start_date)) \n",
    "                & (data['Date'] <= pd.to_datetime(start_date) + pd.DateOffset(days=timeframe))]\n",
    "\n",
    "    # Create a subplot layout with two rows\n",
    "    line_chart = go.Figure()\n",
    "\n",
    "    # Create a subplot layout with two rows\n",
    "    line_chart = make_subplots(rows=1, cols=1, shared_xaxes=True, \n",
    "                            specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "    # Add closing prices and SMA_20 to the first row\n",
    "    line_chart.add_trace(go.Scatter(x=data['Date'], y=data['Close'], mode='lines', name='Close Price', yaxis='y1', line=dict(color='white')))\n",
    "    line_chart.add_trace(go.Scatter(x=data['Date'], y=data['SMA_20'], mode='lines', name='20-day SMA', line=dict(width=1, color='white', dash='dot')))\n",
    "\n",
    "    # Add traces for Bollinger Bands, SMA, and Candlestick\n",
    "    line_chart.add_trace(go.Scatter(x=data['Date'], y=data['Upper_Band'], mode='lines', name='Upper Bollinger Band',\n",
    "                            line=dict(width=1, color=\"blue\")))\n",
    "    line_chart.add_trace(go.Scatter(x=data['Date'], y=data['Lower_Band'], mode='lines', name='Lower Bollinger Band',\n",
    "                            fill='tonexty', fillcolor='rgba(173,216,230, 0.2)', line=dict(width=1, color=\"blue\")))\n",
    "\n",
    "    # Find periods of overbought and oversold to add shapes\n",
    "    for i in range(1, len(data['RSI'])):\n",
    "        if data['RSI'].iloc[i - 1] <= 70 and data['RSI'].iloc[i] > 70:  # Entering overbought\n",
    "            area_start = data['Date'].iloc[i]\n",
    "        elif data['RSI'].iloc[i - 1] > 70 and data['RSI'].iloc[i] <= 70:  # Exiting overbought\n",
    "            line_chart.add_shape(type='rect', x0=area_start, x1=data['Date'].iloc[i], \n",
    "                        y0=data['Close'].min()* 0.95, y1=data['Close'].max()* 1.05,\n",
    "                        line=dict(width=0), fillcolor='red', opacity=0.2)\n",
    "\n",
    "        if data['RSI'].iloc[i - 1] >= 30 and data['RSI'].iloc[i] < 30:  # Entering oversold\n",
    "            area_start = data['Date'].iloc[i]\n",
    "        elif data['RSI'].iloc[i - 1] < 30 and data['RSI'].iloc[i] >= 30:  # Exiting oversold\n",
    "            line_chart.add_shape(type='rect', x0=area_start, x1=data['Date'].iloc[i], \n",
    "                        y0=data['Close'].min()* 0.95, y1=data['Close'].max()* 1.05,\n",
    "                        line=dict(width=0), fillcolor='green', opacity=0.2)\n",
    "\n",
    "    # Add Volatility to the second row (Secondary Y-Axis)\n",
    "    line_chart.add_trace(go.Scatter(x=data['Date'], y=data['Volatility_30'] * 100, mode='lines', name='30-day Volatility (%)', yaxis='y2', line=dict(width=1,color='grey')))\n",
    "\n",
    "    line_chart.update_layout(\n",
    "        showlegend=False,\n",
    "        xaxis=dict(showgrid=False, title=\"\", tickvals=[], showticklabels=False),\n",
    "        yaxis=dict(showgrid=False, tickvals=[], showticklabels=False, side=\"left\", range=[data['Close'].min() * 0.95, data['Close'].max() * 1.05]),\n",
    "        yaxis2=dict(showgrid=False, tickvals=[], showticklabels=False, side=\"right\"),\n",
    "        paper_bgcolor='black',\n",
    "        plot_bgcolor='black'\n",
    "        )\n",
    "\n",
    "    # Scale image based on timeframe\n",
    "    if timeframe == 14:\n",
    "        line_chart.update_layout(\n",
    "            height=300,\n",
    "            width=250\n",
    "        )\n",
    "    elif timeframe == 30:\n",
    "        line_chart.update_layout(\n",
    "            height=300,\n",
    "            width=300\n",
    "        )\n",
    "    elif timeframe == 90:\n",
    "        line_chart.update_layout(\n",
    "            height=300,\n",
    "            width=400\n",
    "        )\n",
    "    elif timeframe == 180:\n",
    "        line_chart.update_layout(\n",
    "            height=300,\n",
    "            width=600\n",
    "        )\n",
    "    else:\n",
    "        line_chart.update_layout(\n",
    "            height=300,\n",
    "            width=800\n",
    "        )\n",
    "\n",
    "    # Save image and label\n",
    "    image_name = save_chart(line_chart, etf, start_date, timeframe, \"Line\")\n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checks if there is a local top detected at curr index\n",
    "def rw_top(data: np.array, curr_index: int, order: int) -> bool:\n",
    "    if curr_index < order * 2 + 1:\n",
    "        return False\n",
    "\n",
    "    top = True\n",
    "    k = curr_index - order\n",
    "    v = data[k]\n",
    "    for i in range(1, order + 1):\n",
    "        if data[k + i] > v or data[k - i] > v:\n",
    "            top = False\n",
    "            break\n",
    "    \n",
    "    return top\n",
    "\n",
    "# Checks if there is a local top detected at curr index\n",
    "def rw_bottom(data: np.array, curr_index: int, order: int) -> bool:\n",
    "    if curr_index < order * 2 + 1:\n",
    "        return False\n",
    "\n",
    "    bottom = True\n",
    "    k = curr_index - order\n",
    "    v = data[k]\n",
    "    for i in range(1, order + 1):\n",
    "        if data[k + i] < v or data[k - i] < v:\n",
    "            bottom = False\n",
    "            break\n",
    "    \n",
    "    return bottom\n",
    "\n",
    "def rw_extremes(data: np.array, order:int):\n",
    "    # Rolling window local tops and bottoms\n",
    "    tops = []\n",
    "    bottoms = []\n",
    "    for i in range(len(data)):\n",
    "        if rw_top(data, i, order):\n",
    "            # top[0] = confirmation index\n",
    "            # top[1] = index of top\n",
    "            # top[2] = price of top\n",
    "            top = [i, i - order, data[i - order]]\n",
    "            tops.append(top)\n",
    "        \n",
    "        if rw_bottom(data, i, order):\n",
    "            # bottom[0] = confirmation index\n",
    "            # bottom[1] = index of bottom\n",
    "            # bottom[2] = price of bottom\n",
    "            bottom = [i, i - order, data[i - order]]\n",
    "            bottoms.append(bottom)\n",
    "    \n",
    "    return tops, bottoms\n",
    "\n",
    "def connect_tops_and_bottoms(tops, bottoms, data):\n",
    "    # Combine tops and bottoms and sort by index\n",
    "    combined = sorted(tops + bottoms, key=lambda x: x[1])\n",
    "    dates = [data.index[point[1]] for point in combined]\n",
    "    prices = [point[2] for point in combined]\n",
    "\n",
    "    return dates, prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_and_connect_doubles(tops_bottoms, time_frame, price_diff, df):\n",
    "    double_patterns = []\n",
    "\n",
    "    for i in range(len(tops_bottoms) - 1):\n",
    "        for j in range(i + 1, len(tops_bottoms)):\n",
    "            date_diff = df.index[tops_bottoms[j][1]] - df.index[tops_bottoms[i][1]]\n",
    "            price_diff_pct = abs(tops_bottoms[j][2] - tops_bottoms[i][2]) / tops_bottoms[i][2] * 100\n",
    "\n",
    "            # Check if the tops/bottoms are within the specified time frame and price difference\n",
    "            if date_diff.days <= time_frame and price_diff_pct < price_diff:\n",
    "                double_patterns.append((tops_bottoms[i], tops_bottoms[j]))\n",
    "\n",
    "    return double_patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two functions to calculate if a level is SUPPORT or a RESISTANCE level through fractal identification\n",
    "def is_Suppport_Level(df, i):\n",
    "    support = df['Low'][i] < df['Low'][i - 1] and df['Low'][i] < df['Low'][i + 1] and df['Low'][i + 1] < df['Low'][i + 2] and df['Low'][i - 1] < df['Low'][i - 2]\n",
    "    return support\n",
    "\n",
    "def is_Resistance_Level(df, i):\n",
    "    resistance = df['High'][i] > df['High'][i - 1] and df['High'][i] > df['High'][i + 1] and df['High'][i + 1] > df['High'][i + 2] and df['High'][i - 1] > df['High'][i - 2]\n",
    "    return resistance\n",
    "\n",
    "# This function, given a price value, returns True or False depending on if it is too near to some previously discovered key level.\n",
    "def distance_from_mean(level, levels, mean):\n",
    "    return np.sum([abs(level - y) < mean for y in levels]) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_breakout_line(fig, pattern, data, time_frame, top):\n",
    "    # pattern[0] and pattern[1] are the two tops/bottoms, pattern[2] is the confirming point (trough/peak)\n",
    "\n",
    "    if top==True:  # For double tops, use the lowest point between tops\n",
    "        confirming_idx = min(range(pattern[0][1], pattern[1][1]), key=lambda x: data.iloc[x]['Close'])\n",
    "    else:  # For double bottoms, use the highest point between bottoms\n",
    "        confirming_idx = max(range(pattern[0][1], pattern[1][1]), key=lambda x: data.iloc[x]['Close'])\n",
    "\n",
    "    breakout_level = data.iloc[confirming_idx]['Close']\n",
    "    start_date = data.index[confirming_idx]  # Start from the second top/bottom\n",
    "    end_date = start_date + pd.Timedelta(days=time_frame)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=[start_date, end_date], y=[breakout_level, breakout_level], mode='lines', line=dict(color='white'), name='Breakout Line'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trading_viz(etf, data, start_date, timeframe): \n",
    "    # Filter the data for the specified date range\n",
    "    data = data[(data['Date'] >= pd.to_datetime(start_date)) \n",
    "                & (data['Date'] <= pd.to_datetime(start_date) + pd.DateOffset(days=timeframe))]\n",
    "\n",
    "    # Initialize Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    fig.add_trace(go.Candlestick(x=data['Date'], open=data['Open'], high=data['High'], low=data['Low'], close=data['Close'],\n",
    "                                    name=\"Candlestick\"))\n",
    "                                    \n",
    "    price_diff = 2  # percentage\n",
    "    data['Date'] = data['Date'].astype('datetime64[s]')\n",
    "    data = data.set_index('Date')\n",
    "\n",
    "    if timeframe == 14:\n",
    "        tops, bottoms = rw_extremes(data['Close'].to_numpy(), 1)\n",
    "        time_frame = 7  # days\n",
    "    elif timeframe == 30:\n",
    "        tops, bottoms = rw_extremes(data['Close'].to_numpy(), 3)\n",
    "        time_frame = 15  # days\n",
    "    elif timeframe == 90:\n",
    "        tops, bottoms = rw_extremes(data['Close'].to_numpy(), 5)\n",
    "        time_frame = 30  # days\n",
    "    else:\n",
    "        tops, bottoms = rw_extremes(data['Close'].to_numpy(), 10)\n",
    "        time_frame = 45  # days\n",
    "    data['Close'].plot()\n",
    "    idx = data.index\n",
    "\n",
    "    # Identify double tops and bottoms\n",
    "    double_tops = identify_and_connect_doubles(tops, time_frame, price_diff, data)\n",
    "    double_bottoms = identify_and_connect_doubles(bottoms, time_frame, price_diff, data)\n",
    "\n",
    "    # Creating a list and feeding it the identified support and resistance levels via the Support and Resistance functions\n",
    "    first_levels = []\n",
    "    first_level_types = []\n",
    "    for i in range(2, data.shape[0] - 2):\n",
    "\n",
    "        if is_Suppport_Level(data, i):\n",
    "            first_levels.append((i, data['Low'][i].round(2)))\n",
    "            first_level_types.append('Support')\n",
    "\n",
    "        elif is_Resistance_Level(data, i):\n",
    "            first_levels.append((i, data['High'][i].round(2)))\n",
    "            first_level_types.append('Resistance')\n",
    "\n",
    "    # Clean noise in data by discarding a level if it is near another\n",
    "    # (i.e. if distance to the next level is less than the average candle size for any given day - this will give a rough estimate on volatility)\n",
    "    mean = np.mean(data['High'] - data['Low'])\n",
    "    \n",
    "    # Optimizing the analysis by adjusting the data and eliminating the noise from volatility that is causing multiple levels to show/overlapp\n",
    "    levels = []\n",
    "    level_types = []\n",
    "    for i in range(2, data.shape[0] - 2):\n",
    "\n",
    "        if is_Suppport_Level(data, i):\n",
    "            level = data['Low'][i].round(2)\n",
    "\n",
    "            if distance_from_mean(level, levels, mean):\n",
    "                levels.append((i, level))\n",
    "                level_types.append('Support')\n",
    "\n",
    "        elif is_Resistance_Level(data, i):\n",
    "            level = data['High'][i].round(2)\n",
    "\n",
    "            if distance_from_mean(level, levels, mean):\n",
    "                levels.append((i, level))\n",
    "                level_types.append('Resistance')\n",
    "\n",
    "    # Plot ETF close prices\n",
    "    #fig.add_trace(go.Scatter(x=data.index, y=data['Close'], mode='lines', line=dict(color='grey', width=1),  name='Close Price'))\n",
    "\n",
    "    # Connect tops and bottoms\n",
    "    connect_dates, connect_prices = connect_tops_and_bottoms(tops, bottoms, data)\n",
    "\n",
    "    # Add the connecting line to the figure\n",
    "    fig.add_trace(go.Scatter(x=connect_dates, y=connect_prices, mode='lines', line=dict(color='white', width=2), name='Trend Line'))\n",
    "\n",
    "    # Plot individual tops as green markers\n",
    "    for top in tops:\n",
    "        fig.add_trace(go.Scatter(x=[data.index[top[1]]], y=[top[2]], mode='markers', marker=dict(color='green', size=3), name='Top'))\n",
    "\n",
    "    # Plot individual bottoms as red markers\n",
    "    for bottom in bottoms:\n",
    "        fig.add_trace(go.Scatter(x=[data.index[bottom[1]]], y=[bottom[2]], mode='markers', marker=dict(color='red', size=3), name='Bottom'))\n",
    "\n",
    "    # Plot connected double tops with green lines\n",
    "    for dt in double_tops:\n",
    "        dt_dates = [data.index[dt[0][1]], data.index[dt[1][1]]]\n",
    "        dt_prices = [dt[0][2], dt[1][2]]\n",
    "        fig.add_trace(go.Scatter(x=dt_dates, y=dt_prices, mode='lines+markers', marker=dict(color='green', size=5), line=dict(color='green'), name='Double Top'))\n",
    "\n",
    "    # Plot connected double bottoms with red lines\n",
    "    for db in double_bottoms:\n",
    "        db_dates = [data.index[db[0][1]], data.index[db[1][1]]]\n",
    "        db_prices = [db[0][2], db[1][2]]\n",
    "        fig.add_trace(go.Scatter(x=db_dates, y=db_prices, mode='lines+markers', marker=dict(color='red', size=5), line=dict(color='red'), name='Double Bottom'))\n",
    "\n",
    "    # Add breakout lines for double tops and bottoms\n",
    "    for dt in double_tops:\n",
    "        add_breakout_line(fig, dt, data, 90, True)\n",
    "\n",
    "    for db in double_bottoms:\n",
    "        add_breakout_line(fig, db, data, 90, False)\n",
    "\n",
    "    # Loop through levels and plot\n",
    "    for level, level_type in zip(levels, level_types):\n",
    "        index, price_level = level\n",
    "        color = 'violet' if level_type == 'Support' else 'blue'\n",
    "        \n",
    "        # Add a horizontal line for each level\n",
    "        fig.add_trace(go.Scatter(x=[data.index[index], data.index[-1]], y=[price_level, price_level], mode='lines', line=dict(color=color, width=0.5), name=level_type))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        xaxis=dict(\n",
    "        showgrid=False,\n",
    "        title=\"\",\n",
    "        tickvals=[], \n",
    "        showticklabels=False\n",
    "        ),\n",
    "        yaxis=dict(\n",
    "        title='',\n",
    "        showgrid=False,\n",
    "        tickvals=[], \n",
    "        showticklabels=False),\n",
    "        xaxis_rangeslider_visible=False,\n",
    "        paper_bgcolor='black',  # Set the background color to black\n",
    "        plot_bgcolor='black',  # Set the plot background color to black\n",
    "        showlegend=False)\n",
    "  \n",
    "    # Scale image based on timeframe\n",
    "    if timeframe == 14:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=250\n",
    "        )\n",
    "    elif timeframe == 30:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=300\n",
    "        )\n",
    "    elif timeframe == 90:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=400\n",
    "        )\n",
    "    elif timeframe == 180:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=600\n",
    "        )\n",
    "    else:\n",
    "        fig.update_layout(\n",
    "            height=300,\n",
    "            width=800\n",
    "        )\n",
    "\n",
    "    # Save image and label\n",
    "    image_name = save_chart(fig, etf, start_date, timeframe, \"AlgoTrading\")\n",
    "    return image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "START = \"2019-02-01\"\n",
    "END = \"2023-12-01\"\n",
    "TIMEFRAMES = [14, 30, 90, 180, 365]\n",
    "PREDICTIONS = [5, 30, 90]\n",
    "STEPS = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate ETFs\n",
    "for key in etfs:\n",
    "    print(key)\n",
    "    # Iterate timeframes\n",
    "    for tf in TIMEFRAMES:\n",
    "        start_date = pd.to_datetime(START)\n",
    "\n",
    "        # Create images multiple times\n",
    "        # While timeframe is possible to chart\n",
    "        while start_date + pd.DateOffset(days=tf) < pd.to_datetime(END):\n",
    "            # Create chart image\n",
    "            image_name = build_black_white_ohlc(key, etfs[key], start_date, tf)\n",
    "            \n",
    "            # Set end date of chart\n",
    "            end_date = start_date + pd.DateOffset(days=tf)\n",
    "\n",
    "            # Iterate prediction timeframe\n",
    "            for prediction in PREDICTIONS:\n",
    "\n",
    "                # Only predict if prediction timesframe is smaller than chart timeframe\n",
    "                if prediction <= tf:\n",
    "                    # Save label in csv file\n",
    "                    save_label(etfs[key], image_name, start_date, prediction)\n",
    "            \n",
    "            start_date = start_date + pd.DateOffset(days=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate ETFs\n",
    "for key in etfs:\n",
    "    print(key)\n",
    "    # Iterate timeframes\n",
    "    for tf in TIMEFRAMES:\n",
    "        start_date = pd.to_datetime(START)\n",
    "\n",
    "        # Create images multiple times\n",
    "        # While timeframe is possible to chart\n",
    "        while start_date + pd.DateOffset(days=tf) <= pd.to_datetime(END):\n",
    "            # Create chart image\n",
    "            image_name = build_colored_ohlc(key, etfs[key], start_date, tf)\n",
    "            \n",
    "            # Set end date of chart\n",
    "            end_date = start_date + pd.DateOffset(days=tf)\n",
    "\n",
    "            # Iterate prediction timeframe\n",
    "            for prediction in PREDICTIONS:\n",
    "\n",
    "                # Only predict if prediction timesframe is smaller than chart timeframe\n",
    "                if prediction <= tf:\n",
    "                    # Save label in csv file\n",
    "                    save_label(etfs[key], image_name, start_date, prediction)\n",
    "            \n",
    "            # Set start_date\n",
    "            start_date = start_date + pd.DateOffset(days=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SmallCap\n",
      "2019-02-11 00:00:00\n",
      "2019-04-11 00:00:00\n",
      "2019-06-01 00:00:00\n",
      "2019-07-31 00:00:00\n",
      "2019-10-01 00:00:00\n",
      "2019-12-12 00:00:00\n",
      "2020-01-27 00:00:00\n",
      "2020-03-27 00:00:00\n",
      "2020-06-03 00:00:00\n",
      "2020-08-07 00:00:00\n",
      "2020-09-23 00:00:00\n",
      "2020-12-03 00:00:00\n",
      "2021-01-21 00:00:00\n",
      "2021-03-22 00:00:00\n",
      "2021-06-02 00:00:00\n",
      "2021-07-20 00:00:00\n",
      "2021-09-18 00:00:00\n",
      "2021-11-26 00:00:00\n",
      "2022-01-19 00:00:00\n",
      "2022-03-17 00:00:00\n",
      "2022-05-16 00:00:00\n",
      "2022-07-27 00:00:00\n",
      "2022-09-27 00:00:00\n",
      "2022-11-12 00:00:00\n",
      "2023-01-23 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-05-11 00:00:00\n",
      "2023-07-14 00:00:00\n",
      "2023-09-20 00:00:00\n",
      "2023-11-14 00:00:00\n",
      "2019-02-27 00:00:00\n",
      "2019-04-11 00:00:00\n",
      "2019-06-20 00:00:00\n",
      "2019-07-31 00:00:00\n",
      "2019-10-25 00:00:00\n",
      "2019-12-20 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-03-27 00:00:00\n",
      "2020-06-03 00:00:00\n",
      "2020-08-07 00:00:00\n",
      "2020-10-22 00:00:00\n",
      "2020-12-18 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-03-22 00:00:00\n",
      "2021-06-08 00:00:00\n",
      "2021-07-20 00:00:00\n",
      "2021-09-18 00:00:00\n",
      "2021-11-26 00:00:00\n",
      "2022-01-19 00:00:00\n",
      "2022-03-17 00:00:00\n",
      "2022-05-16 00:00:00\n",
      "2022-08-11 00:00:00\n",
      "2022-09-29 00:00:00\n",
      "2022-11-12 00:00:00\n",
      "2023-02-07 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-05-11 00:00:00\n",
      "2023-07-25 00:00:00\n",
      "2023-10-04 00:00:00\n",
      "2019-04-11 00:00:00\n",
      "2019-06-20 00:00:00\n",
      "2019-06-20 00:00:00\n",
      "2019-10-25 00:00:00\n",
      "2019-12-20 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-06-03 00:00:00\n",
      "2020-08-07 00:00:00\n",
      "2020-10-22 00:00:00\n",
      "2020-12-18 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-06-08 00:00:00\n",
      "2021-07-16 00:00:00\n",
      "2021-09-20 00:00:00\n",
      "2021-11-26 00:00:00\n",
      "2022-01-19 00:00:00\n",
      "2022-02-16 00:00:00\n",
      "2022-05-09 00:00:00\n",
      "2022-08-11 00:00:00\n",
      "2022-09-29 00:00:00\n",
      "2022-10-28 00:00:00\n",
      "2023-02-07 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-07-25 00:00:00\n",
      "2023-10-04 00:00:00\n",
      "2019-06-20 00:00:00\n",
      "2019-09-13 00:00:00\n",
      "2019-11-07 00:00:00\n",
      "2020-01-17 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-06-26 00:00:00\n",
      "2020-09-22 00:00:00\n",
      "2020-11-16 00:00:00\n",
      "2021-01-19 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-07-16 00:00:00\n",
      "2021-07-16 00:00:00\n",
      "2021-11-15 00:00:00\n",
      "2022-01-04 00:00:00\n",
      "2022-02-16 00:00:00\n",
      "2022-05-09 00:00:00\n",
      "2022-06-28 00:00:00\n",
      "2022-09-01 00:00:00\n",
      "2022-10-28 00:00:00\n",
      "2022-12-19 00:00:00\n",
      "2023-03-08 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-03-23 00:00:00\n",
      "2023-08-15 00:00:00\n",
      "2023-10-30 00:00:00\n",
      "2020-01-17 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-02-26 00:00:00\n",
      "2020-07-29 00:00:00\n",
      "2020-09-22 00:00:00\n",
      "2020-11-16 00:00:00\n",
      "2021-01-19 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-02-19 00:00:00\n",
      "2021-07-16 00:00:00\n",
      "2021-09-20 00:00:00\n",
      "2021-11-15 00:00:00\n",
      "2022-01-19 00:00:00\n",
      "2022-02-16 00:00:00\n",
      "2022-05-09 00:00:00\n",
      "2022-06-28 00:00:00\n",
      "2022-09-15 00:00:00\n",
      "2022-10-28 00:00:00\n",
      "2023-01-13 00:00:00\n",
      "2023-03-17 00:00:00\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/jx/r9zzxsjd7wxgmmn32r0l8cz40000gn/T/ipykernel_3058/1834317169.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_datetime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0;31m# Create chart image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_ma_vol_rsi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0;31m# Set end date of chart\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jx/r9zzxsjd7wxgmmn32r0l8cz40000gn/T/ipykernel_3058/3300794638.py\u001b[0m in \u001b[0;36mbuild_ma_vol_rsi\u001b[0;34m(etf, data, start_date, timeframe)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# Save image and label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0mimage_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_chart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline_chart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimage_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/jx/r9zzxsjd7wxgmmn32r0l8cz40000gn/T/ipykernel_3058/3232080340.py\u001b[0m in \u001b[0;36msave_chart\u001b[0;34m(fig, etf, start_date, timeframe, chart_type)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"images/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchart_type\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0metf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".png\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Save the PNG image to the specified file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/aiss/lib/python3.8/site-packages/plotly/basedatatypes.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3778\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3779\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3780\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/aiss/lib/python3.8/site-packages/plotly/io/_kaleido.py\u001b[0m in \u001b[0;36mto_image\u001b[0;34m(fig, format, width, height, scale, validate, engine)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0mfig_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_coerce_fig_to_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m     img_bytes = scope.transform(\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0mfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/aiss/lib/python3.8/site-packages/kaleido/scopes/plotly.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, figure, format, width, height, scale)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Transform in using _perform_transform rather than superclass so we can access the full\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# response dict, including error codes.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         response = self._perform_transform(\n\u001b[0m\u001b[1;32m    154\u001b[0m             \u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         )\n",
      "\u001b[0;32m~/opt/anaconda3/envs/aiss/lib/python3.8/site-packages/kaleido/scopes/base.py\u001b[0m in \u001b[0;36m_perform_transform\u001b[0;34m(self, data, **kwargs)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_proc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0mresponse_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Iterate ETFs\n",
    "for key in etfs:\n",
    "    print(key)\n",
    "    # Iterate timeframes\n",
    "    for tf in TIMEFRAMES:\n",
    "        start_date = pd.to_datetime(START)\n",
    "\n",
    "        # Create images multiple times\n",
    "        # While timeframe is possible to chart\n",
    "        while start_date + pd.DateOffset(days=tf) < pd.to_datetime(END):\n",
    "            # Create chart image\n",
    "            image_name = build_ma_vol_rsi(key, etfs[key], start_date, tf)\n",
    "            \n",
    "            # Set end date of chart\n",
    "            end_date = start_date + pd.DateOffset(days=tf)\n",
    "\n",
    "            # Iterate prediction timeframe\n",
    "            for prediction in PREDICTIONS:\n",
    "\n",
    "                # Only predict if prediction timesframe is smaller than chart timeframe\n",
    "                if prediction <= tf:\n",
    "                    # Save label in csv file\n",
    "                    save_label(etfs[key], image_name, start_date, prediction)\n",
    "            \n",
    "            # Set start_date\n",
    "            start_date = start_date + pd.DateOffset(days=STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate ETFs\n",
    "for key in etfs:\n",
    "    print(key)\n",
    "    # Iterate timeframes\n",
    "    for tf in TIMEFRAMES:\n",
    "        start_date = pd.to_datetime(START)\n",
    "\n",
    "        # Create images multiple times\n",
    "        # While timeframe is possible to chart\n",
    "        while start_date + pd.DateOffset(days=tf) < pd.to_datetime(END):\n",
    "            # Create chart image\n",
    "            image_name = build_trading_viz(key, etfs[key], start_date, tf)\n",
    "            \n",
    "            # Set end date of chart\n",
    "            end_date = start_date + pd.DateOffset(days=tf)\n",
    "\n",
    "            # Iterate prediction timeframe\n",
    "            for prediction in PREDICTIONS:\n",
    "\n",
    "                # Only predict if prediction timesframe is smaller than chart timeframe\n",
    "                if prediction <= tf:\n",
    "                    # Save label in csv file\n",
    "                    save_label(etfs[key], image_name, start_date, prediction)\n",
    "            \n",
    "            # Set start_date\n",
    "            start_date = start_date + pd.DateOffset(days=STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aiss",
   "language": "python",
   "name": "aiss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
